un2 <- read.csv("un_final.csv") # UN data
# Maps
install.packages("rnaturalearthdata", repos = "https://cloud.r-project.org/")
world <- ne_countries(scale = "medium", returnclass = "sf") # creating map
world <- st_transform(world, crs = 4326) # setting crs
world <- st_make_valid(world)
map_un <- world %>%
left_join(un2, by = c("adm0_iso" = "ccodealp"))
# Aliases for metric names
metric_aliases <- c(
"GDP Per Capita (2015 USD)"= "GDP.per.capita..constant.2015.US..",
"Gini" ="Gini.index",
"Non-Western Nation" = "nonwest",
"Major Power" = "major_power",
"Regime Type" = "regime_status_name",
"Women Empowerment Score" = "vdem_gender",
"Democratic Country" = "dem_bi",
"How close is speech to fostering International Order?" = "IdealPointAll"
)
# Define UI
ui <- fluidPage(
# Application title
titlePanel("UN General Assembly Speeches: A Temporal Analysis of Topics and Sentiment by Country"),
# Sidebar with a slider input for number of bins
tabsetPanel(  # Create a tabset panel
tabPanel("Sentiment and Topics Over Time",  # First tab
p("Choose a topic to see its frequency and sentiment trends"),
(sidebarLayout(
sidebarPanel(
selectInput("topic_choice", "Choose a Topic:", choices = sort(unique(un2$topic_title)))
),
mainPanel(
plotOutput("topic_trend_plot")
)
)
)
),
tabPanel("Topic Instances",  # Second tab
p("Choose a country to see the number of Topics covered by their UNGA Speeches"),
(sidebarLayout(
sidebarPanel(
selectInput("country_topic_choice", "Choose a Country:", choices = sort(unique(un2$country_name)))
),
mainPanel(
plotOutput("topic_count_plot")
)
)
)
),
tabPanel("Map: Sentiment by Year",  # Third tab
p("Choose a year to see a world map of countries by their sentiment score"),
(sidebarLayout(
sidebarPanel(
selectInput("year_choice", "Choose a Year:", choices = sort(unique(un2$year)))
),
mainPanel(
plotOutput("sent_map")
)
)
)
),
tabPanel("Map: Topics by Year",  # Third tab
p("Choose a year and hover to see a world map of countries by topic and metric"),
(sidebarLayout(
sidebarPanel(
selectInput("year_topic_choice", "Choose a Year:", choices = sort(unique(un2$year)))#,
#selectInput("metric_topic_choice", "Choose a Metric:", choices = metric_aliases)
),
mainPanel(
leafletOutput("topic_map2")
)
)
)
),
)
)
# Define server logic required to draw a histogram
server <- function(input, output) {
## Topic and sentiment trend
output$topic_trend_plot <- renderPlot({
un_long <- un2 %>% filter(topic_title==input$topic_choice) %>%
group_by(year) %>% summarize(n_topic = n(),
mean_sent = mean(lsd_net_sent)) %>%
pivot_longer(cols = c(mean_sent, n_topic), names_to = "variable", values_to = "value")
ggplot(un_long, aes(x = year, y = value, color = variable)) +
geom_line() +
scale_color_discrete(name = "Metric", labels = c("Mean Sentiment", "Number of Speeches")) +
labs(x = "Year", y = "Value")
})
## Topic counts by country
output$topic_count_plot <- renderPlot({
ggplot(un2 %>% filter(country_name == input$country_topic_choice) %>%
group_by(topic_title) %>% summarize(n_topic = n()), aes(x = topic_title, y = n_topic)) +
geom_col(fill='lightblue') +
scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +
labs(x = "Topic", y = "Count")})
## Map showing sentiment by year
output$sent_map <- renderPlot({
ggplot(map_un %>% filter(year == input$year_choice) %>% group_by(country_name) %>%
summarize(mean_sent = mean(lsd_net_sent))) +
geom_sf(aes(fill = mean_sent), color = "black") +  # Color countries by 'value'
scale_fill_gradient2(low = "red", mid = "white", high = "green", midpoint = 0) +
labs(title = "Sentiment by Country by Year",
fill = "Sentiment (LSD) Score") +  # Legend title
theme(axis.text = element_blank())})
## Map showing countries by topic by year
#selected_metric <- names(metric_aliases)[metric_aliases == input$metric_topic_choice]
output$topic_map <- renderPlot({
ggplot(map_un %>% filter(year == input$year_topic_choice)) +
geom_sf(aes(fill = factor(topic_title)), color = "black") +  # Color countries by 'value'
#geom_text(aes(label = input$metric_topic_choice, geometry = geometry), stat = "sf_coordinates", size = 3, color = "black") +  # Overlay sentiment values
labs(title = "Countries and Topics by Year",
fill = "Topic") +  # Legend title
theme(axis.text = element_blank(), # remove lat long labels
axis.title = element_blank())})
output$topic_map2 <- renderLeaflet({
# First verify the necessary columns exist
required_columns <- c("topic_title", "year", #input$metric_topic_choice,
"GDP.per.capita..constant.2015.US..", "Gini.index", "regime_status_name")  # Update with your actual column names
validate(
need(all(required_columns %in% names(map_un)),
"Some required columns are missing from the data"
))
filtered_data <- map_un %>% filter(year == input$year_topic_choice)
# Get the correct country name column (update "NAME" to your actual column name)
country_col <- "country_name"  # Change this to your actual country name column
pal <- colorFactor("Set3", domain = filtered_data$topic_title)
# OPTION 2: Manual color assignment (more control)
# First get your unique topics
unique_topics <- sort(unique(filtered_data$topic_title))
# Create a named color vector
topic_colors <- setNames(
c("#2ca02c", "#8c564b", "#7f7f7f", "#98df8a", "#17becf",
"#7f7f7f", "#e377c2", "#bcbd22", "#9edae5", "#d62728"
),
unique_topics
)
pal <- colorFactor(palette = topic_colors,
domain = filtered_data$topic_title)
leaflet(filtered_data) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
addPolygons(
fillColor = ~pal(topic_title),
weight = 1,
opacity = 1,
color = "white",
fillOpacity = 0.7,
highlight = highlightOptions(
weight = 2,
color = "#666",
bringToFront = TRUE),
label = ~sprintf(
"<div style='font-size:14px;line-height:1.4'>
<strong>%s</strong><br/>
GDP Per Capita (2015 USD): %.2f<br/>
Gini: %.2f<br/>
Regime Type: %s
</div>",
get(country_col),  # Country name
ifelse(is.na(GDP.per.capita..constant.2015.US..), NA, GDP.per.capita..constant.2015.US..),  # GDP per capita
ifelse(is.na(Gini.index), NA, Gini.index),  # Gini coefficient
regime_status_name  # Regime type
) %>% lapply(htmltools::HTML),
labelOptions = labelOptions(
style = list(
"font-weight" = "normal",
padding = "6px 10px",
"max-width" = "300px"
),
textsize = "14px",
direction = "auto"
)
) %>%
addLegend(
pal = pal,
values = ~topic_title,
title = "Topics",
position = "bottomright"
)
})
}
# Run the application
shinyApp(ui = ui, server = server)
getwd()
# Installing libraries
# Installing libraries
require(ggplot2)
require(forcats)
require(dplyr)
require(stargazer)
require(MLmetrics)
require(irr)
library(readr)
library(tidyr)
library(tidyverse)
library(broom)
library(pacman)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(pdftools)
library(tibble)
library(sentimentr)
library(topicmodels)
library(tidytext)
library(vader)
library(e1071)  # Load the package
library(tidymodels)
# Installing libraries
# Installing libraries
require(ggplot2)
require(forcats)
require(dplyr)
require(stargazer)
require(MLmetrics)
require(irr)
library(readr)
library(tidyr)
library(tidyverse)
library(broom)
library(pacman)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(pdftools)
library(tibble)
library(sentimentr)
library(topicmodels)
library(tidytext)
library(vader)
library(e1071)  # Load the package
library(tidymodels)
install.library("tidymodels")
install.libraries("tidymodels")
library(wesanderson)
library(glmnet)
library(text2vec)
library(conText)
pacman::p_load(readtext)
pacman::p_load(quanteda.textplots, quanteda.textstats)
pacman::p_load("quanteda.dictionaries", "quanteda.sentiment")
pacman::p_load(tidyverse, quanteda, quanteda.corpora, quanteda.textstats, quanteda.textmodels, rjson, caret)
set.seed(10)
# Raw data for women's march
wm_raw <- read.csv("Dataverse/data/FelmleeEtAl_corpus.csv", stringsAsFactors = F)
# sampled 20k data for women's march
wm_sample <- read.csv("Dataverse/data/WM_tweets_groundtruth.csv")
# tweet scores by user
wm_scores <- read.csv("Dataverse/data/WM_tweets_analysis_tweetscores.csv")
# Creating corpus
wm_raw_corpus <- corpus(wm_raw, text_field = "tweets")
# Preprocessing
tokens <- tokens(wm_raw_corpus,
split_hyphens = FALSE, # keep hyphenated words
to_lower = TRUE, # convert all to lowercase
remove_punct = TRUE, # remove punctuation
remove_numbers = TRUE, # remove digits
remove_symbols = TRUE, # remove symbols
remove_url = TRUE, # remove links
valuetype = "regex") %>%
tokens_tolower() %>% # to lowercase
tokens_remove(stopwords("en")) # remove stopwords, "@",
tokens <- tokens_wordstem(tokens) # stem
# DFM
dfm <- dfm(tokens)
# re-importing VADER scores
wm_raw <- read.csv("wm_raw_vader_rep.csv")
# Comparing replication VADER scores to original
wm_raw$vader_rep %>% summary() # VADER replication composite values
wm_raw$sentiment_untargeted %>% summary() # VADER values from paper
# Create the boxplot with own VADER scores
ggplot(wm_raw %>% filter(place %in% c("washington","newyork","boston","losangeles","chicago","denver")), aes(x = reorder(place, vader_rep, FUN = mean), y = vader_rep)) +
geom_boxplot() +
labs(
title = "Boxplot of VADER Sent. Scores by City",
x = "City",
y = "VADER Sent. Score"
) +
ylim(-4,4)+
theme_minimal()
# Create the boxplot with original research VADER scores
ggplot(wm_raw %>% filter(place %in% c("washington","newyork","boston","losangeles","chicago","denver")), aes(x = reorder(place, sentiment_untargeted, FUN = mean), y = sentiment_untargeted)) +
geom_boxplot() +
labs(
title = "Boxplot of VADER Sent. Scores by City",
x = "City",
y = "VADER Sent. Score"
) +
ylim(-4,4)+
theme_minimal()
# re-importing VADER scores
wm_sample <- read.csv("wm_sample_vader_rep.csv")
# Comparing replication VADER scores for 20k sample to original
print("Replicated VADER Scores:")
wm_sample$vader_rep %>% summary()
cat("\n")
print("Original VADER Scores:")
wm_sample$vader_scores %>% summary()
wm_sample_merge <- wm_sample %>% inner_join(wm_raw, by = c("text" = "tweets"))
# Create the boxplot with original research VADER scoresof 20k sample
ggplot(wm_sample_merge %>% filter(place %in% c("washington","newyork","boston","losangeles","chicago","denver")), aes(x = reorder(place, sentiment_untargeted, FUN = mean), y = sentiment_untargeted)) +
geom_boxplot() +
labs(
title = "Boxplot of VADER Sent. Scores by City",
x = "City",
y = "VADER Sent. Score"
) +
ylim(-4,4)+
theme_minimal()
# Cross tab: sentiment and stance in 20k sample
xtabs(~ stance + sentiment, data = wm_sample)
# ideology effect on sentiment
lr_vader_ideol <- glm(vader_sentiment ~ ideology_score,  family=binomial, data = wm_scores)
# ideology effect on stance (bert)
lr_bert_ideol <- glm(bert_stance ~ ideology_score, family=binomial, data = wm_scores)
# ideology effect on stance (hand-coded)
lr_human_ideol <- glm(stance ~ ideology_score, family=binomial,data = wm_scores)
# Tidying with Stargazer
stargazer(lr_vader_ideol, lr_bert_ideol, lr_human_ideol,
type = "text",
title = "Logistic Regression Replication Results",
#dep.var.labels = "Dependent Variable: vs",
covariate.labels = c("Ideology (lib-con)", "Constant"),
column.labels = c("VADER Sent.", "BERT Stance", "Human-coded Stance"),
digits = 3,
omit.stat = c("LL", "ser"),
star.cutoffs = c(0.05, 0.01, 0.001))
# X train - Creating training data set (random 80% of tokens)
sampled_indices <- sample(ndoc(dfm), size = round(ndoc(dfm) * 0.8))
training_data <- dfm[sampled_indices, ]
# X test
test_data <- dfm[-sampled_indices, ]
# y train - Creating training data lables (indexing for randomly sampled docs)
train_labels <- wm_raw$bert_stance[sampled_indices]
# y test
test_labels <- wm_raw$bert_stance[-sampled_indices]
# Training Naive Bayes model
nb_wm <- textmodel_nb(training_data, train_labels, smooth = 1, prior = c("docfreq"))
# Make predictions on the new message
predicted_labels <- predict(nb_wm, newdata = test_data) # pred
true_labels <- test_labels # true
# Confusion matrix
conf_matrix <- table(Predicted = predicted_labels, Actual = true_labels)
conf_matrix
# Calculating metrics
nb_acc <- sum(diag(conf_matrix)) / sum(conf_matrix)  # accuracy
nb_recall <- conf_matrix[2, 2] / sum(conf_matrix[2, ])  # recall
nb_precision <- conf_matrix[2, 2] / sum(conf_matrix[, 2])  # precision
nb_f1 <- 2 * (nb_recall * nb_precision) / (nb_recall + nb_precision)  # F1
# Displaying metrics
cat(
"Accuracy:", nb_acc, "\n",
"Recall:", nb_recall, "\n",
"Precision:", nb_precision, "\n",
"F1-score:", nb_f1
)
# ---- Step 4: Train a Lasso Model with Cross-Validation
lasso <- cv.glmnet(
x = as(train_data, "dgCMatrix"), y = train_labels,  # Convert dfm to matrix
family = "binomial", alpha = 1, nfolds = 5,  # Logistic regression (binomial); Lasso (alpha = 1); 10-fold Cross-Validation
intercept = TRUE, type.measure = "class"
)
# ---- Step 4: Train a Lasso Model with Cross-Validation
lasso <- cv.glmnet(
x = as(training_data, "dgCMatrix"), y = train_labels,  # Convert dfm to matrix
family = "binomial", alpha = 1, nfolds = 5,  # Logistic regression (binomial); Lasso (alpha = 1); 10-fold Cross-Validation
intercept = TRUE, type.measure = "class"
)
# ---- Step 6: Predict on Test Set
predicted_lasso <- predict(lasso, newx = test_data, type = "class")
# ---- Step 7: Evaluate Performance with Confusion Matrix ----
conf_matrix_lasso <- table(Predicted = predicted_lasso, Actual = test_labels)
conf_matrix_lasso
# Calculating metrics
lasso_acc <- sum(diag(conf_matrix_lasso)) / sum(conf_matrix_lasso)  # accuracy
lasso_recall <- conf_matrix_lasso[2, 2] / sum(conf_matrix_lasso[2, ])  # recall
lasso_precision <- conf_matrix_lasso[2, 2] / sum(conf_matrix_lasso[, 2])  # precision
lasso_f1 <- 2 * (nb_recall * nb_precision) / (nb_recall + nb_precision)  # F1
# Displaying metrics
cat(
"Accuracy:", lasso_acc, "\n",
"Recall:", lasso_recall, "\n",
"Precision:", lasso_precision, "\n",
"F1-score:", lasso_f1
)
# get coefficients in the best model (with lowest lambda)
coef_df <- coef(lasso, s = lasso$lambda.min) %>%
as.matrix() %>% as.data.frame()
colnames(coef_df) <- "coefficient"
# create feature variable using row names
coef_df$feature <- rownames(coef_df)
# remove intercept
coef_df <- coef_df[coef_df$feature != "(Intercept)", ]
# Top words with positive coefficients
head(coef_df[order(coef_df$coefficient, decreasing = TRUE), ], 10)
# Top words with negative coefficients
head(coef_df[order(coef_df$coefficient), ], 10)
pacman::p_load(data.table,
cowplot,
latex2exp)
# transformation matrix
local_fasttext = readRDS("fasttext_transform_enwiki_50.rds")
# Creating corpus
wm_sample <- wm_sample %>%
mutate(attitude = if_else(stance == 1, "supporter", "opponent"))
corp <- corpus(wm_sample, text_field = "text")
# remove short speeches
corpus <- corp %>%
corpus_trim(what = "documents",
min_ntoken = 5)
# some pre-processing
toks <- tokens(corpus,
remove_punct=T,
remove_symbols=T,
remove_numbers=T,
remove_url=T) %>%
tokens_tolower()
# without stops (also works with them!)
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")
# only use features that appear at least 10 times in the corpus
feats <- dfm(toks_nostop, tolower=T, verbose = FALSE) %>%
dfm_trim(min_termfreq = 10) %>% featnames()
toks_nostop <- tokens_select(toks_nostop, feats, padding = TRUE)
wm_toks <- tokens_context(x = toks_nostop, pattern = "*march*", window = 5L)
wm_dfm <- dfm(wm_toks)
# create document-embedding matrix using our locally trained GloVe embeddings and transformation matrix
wm_dem_local <- dem(x = wm_dfm, pre_trained = word_vectors,
transform = TRUE, transform_matrix = local_fasttext,
verbose = TRUE)
# token object around target words
target_toks <- tokens_context(x = toks_nostop, pattern = "*march*", window = 5L)
feats <- featnames(dfm(target_toks)) # for restrictions of candidates
# nearest neighbors: features with the highest cosine-similarity with each group embedding
# ---------------------------------
# by government vs. opposition
target_nns <- get_nns(x = target_toks, N = 10,
groups = docvars(target_toks, 'attitude'),
candidates = feats, # restrict to candidates in context window
pre_trained = word_vectors,
transform = TRUE,
transform_matrix = local_fasttext,
bootstrap = F) %>%
lapply(., "[[",2) %>%
do.call(rbind, .) %>%
as.data.frame()
plotfun <- function(n){
temp <- tokens_subset(target_toks)
feats <- featnames(dfm(target_toks))
set.seed(111)
target_nns_ratio <- get_nns_ratio(x = temp,
N = n,
groups = docvars(temp, 'attitude'),
numerator = "supporter",
candidates = feats,
pre_trained = word_vectors,
transform = TRUE,
transform_matrix = local_fasttext,
bootstrap = T,
num_bootstraps = 100,
permute = TRUE,
num_permutations = 100,
verbose = T)
return(target_nns_ratio)
}
nb_by_group <- plotfun(n = 10)
# token object around target words
target_toks <- tokens_context(x = toks_nostop, pattern = "*trump*", window = 5L)
feats <- featnames(dfm(target_toks)) # for restrictions of candidates
# nearest neighbors: features with the highest cosine-similarity with each group embedding
# ---------------------------------
# by government vs. opposition
target_nns <- get_nns(x = target_toks, N = 10,
groups = docvars(target_toks, 'attitude'),
candidates = feats, # restrict to candidates in context window
pre_trained = word_vectors,
transform = TRUE,
transform_matrix = local_fasttext,
bootstrap = F) %>%
lapply(., "[[",2) %>%
do.call(rbind, .) %>%
as.data.frame()
# transformation matrix
local_fasttext = readRDS("fasttext_transform_enwiki_50.rds")
install.packages("tidymodels")
install.packages("tidymodels")
install.packages("fastText")
# Installing libraries
# Installing libraries
require(ggplot2)
require(forcats)
require(dplyr)
require(stargazer)
require(MLmetrics)
require(irr)
library(readr)
library(tidyr)
library(tidyverse)
library(broom)
library(pacman)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(pdftools)
library(tibble)
library(sentimentr)
library(topicmodels)
library(tidytext)
library(vader)
library(e1071)  # Load the package
library(tidymodels)
library(tidymodels)
update.packages(ask = FALSE, checkBuilt = TRUE)
